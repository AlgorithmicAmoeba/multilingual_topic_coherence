{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(f\"{data_directory}/data.parquet\").to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = np.array([d[\"bow\"] for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_directory}/vocabulary.json\") as f:\n",
    "    vocabulary = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_LANGUAGES = len(set(d[\"language\"] for d in data))\n",
    "N_LANGUAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_directory}/output.jsonl\") as f:\n",
    "    output = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPMI scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create word-frequency matrix for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90000 [00:00<?, ?it/s]/home/darren/.local/share/virtualenvs/phd_code-idiVf7sk/lib/python3.10/site-packages/scipy/sparse/_index.py:102: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "100%|██████████| 90000/90000 [22:55<00:00, 65.45it/s] \n"
     ]
    }
   ],
   "source": [
    "# count unique words in clean_words\n",
    "words = set(itertools.chain.from_iterable(d[\"clean_words\"] for d in data))\n",
    "word2idx = {word: idx for idx, word in enumerate(words)}\n",
    "\n",
    "# create sparse word-doc matrix\n",
    "import scipy.sparse\n",
    "\n",
    "doc_word = scipy.sparse.csr_matrix((len(data), len(words)), dtype=np.uint8)\n",
    "\n",
    "for i, d in enumerate(tqdm.tqdm(data)):\n",
    "    for word in d[\"clean_words\"]:\n",
    "        doc_word[i, word2idx[word]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word2idx and doc_word to disk\n",
    "import pickle\n",
    "\n",
    "with open(f\"{data_directory}/word2idx.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word2idx, f)\n",
    "\n",
    "scipy.sparse.save_npz(f\"{data_directory}/doc_word.npz\", doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2idx and doc_word from disk\n",
    "import pickle\n",
    "\n",
    "with open(f\"{data_directory}/word2idx.pkl\", \"rb\") as f:\n",
    "    word2idx = pickle.load(f)\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "doc_word = scipy.sparse.load_npz(f\"{data_directory}/doc_word.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word co-occurrence calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=None)\n",
    "def p1(word):\n",
    "    if word in vocabulary:\n",
    "        value = p1_vocabulary(word)\n",
    "    else:\n",
    "        value = p1_all_words(word)\n",
    "    return value\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def p2(word1, word2):\n",
    "    if word1 not in vocabulary or word2 not in vocabulary:\n",
    "        value = p2_all_words(word1, word2)\n",
    "    else:\n",
    "        value = p2_vocabulary(word1, word2)\n",
    "    return value\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def p1_vocabulary(word):\n",
    "    value = np.count_nonzero(bow[:, vocabulary[word][\"idx\"]]) / bow.shape[0]\n",
    "    return value\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def p2_vocabulary(word1, word2):\n",
    "    value = np.count_nonzero(np.logical_and(bow[:, vocabulary[word1][\"idx\"]], bow[:, vocabulary[word2][\"idx\"]])) / bow.shape[0]\n",
    "    return value\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def p1_all_words(word):\n",
    "    if word not in word2idx:\n",
    "        return 0\n",
    "    value = doc_word[:, word2idx[word]].count_nonzero() / doc_word.shape[0]\n",
    "    return value\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def p2_all_words(word1, word2):\n",
    "    if word1 not in word2idx or word2 not in word2idx:\n",
    "        return 0\n",
    "    value = doc_word[:, word2idx[word1]].multiply(doc_word[:, word2idx[word2]]).count_nonzero()  / doc_word.shape[0]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npmi_word(word1, word2):\n",
    "    joint = p2(word1, word2)\n",
    "    p1w1 = p1(word1)\n",
    "    p1w2 = p1(word2)\n",
    "    if joint == 0:\n",
    "        return 0\n",
    "    value = np.log(p2(word1, word2) / (p1w1 * p1w2)) / -np.log(p2(word1, word2))\n",
    "    return value\n",
    "\n",
    "def npmi_topic(topic):\n",
    "    return np.mean([\n",
    "        npmi_word(word1, word2)\n",
    "        for word1, word2 in itertools.combinations(topic, 2)\n",
    "    ])\n",
    "\n",
    "\n",
    "def npmi_topics(topics):\n",
    "    return np.mean([\n",
    "        npmi_topic(topic)\n",
    "        for topic in topics\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darren/.local/share/virtualenvs/phd_code-idiVf7sk/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sonar.inference_pipelines.text\n",
    "\n",
    "t2vec_model = sonar.inference_pipelines.text.TextToEmbeddingModelPipeline(\n",
    "    encoder=\"text_sonar_basic_encoder\",\n",
    "    tokenizer=\"text_sonar_basic_encoder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topic_words = set(\n",
    "    word\n",
    "    for output_i in output\n",
    "    for topic in output_i[\"topics\"]\n",
    "    for word in topic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_langauges = collections.defaultdict(set)\n",
    "\n",
    "for data_i in data:\n",
    "    for word in data_i[\"clean_words\"]:\n",
    "        if word in all_topic_words:\n",
    "            topic_words_langauges[word].add(data_i[\"language\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_topic_words = {\n",
    "    language: set(word for word, languages in topic_words_langauges.items() if language in languages)\n",
    "    for language in set(d[\"language\"] for d in data)\n",
    "}\n",
    "\n",
    "# covert sets to lists\n",
    "language_topic_words = {\n",
    "    language: list(words)\n",
    "    for language, words in language_topic_words.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings: 100%|██████████| 5/5 [00:48<00:00,  9.63s/it]\n",
      "Embeddings: 100%|██████████| 4/4 [00:35<00:00,  8.95s/it]\n",
      "Embeddings: 100%|██████████| 4/4 [00:31<00:00,  7.98s/it]\n",
      "Embeddings: 100%|██████████| 3/3 [00:22<00:00,  7.40s/it]\n",
      "Embeddings: 100%|██████████| 4/4 [00:35<00:00,  8.94s/it]\n",
      "Embeddings: 100%|██████████| 5/5 [00:40<00:00,  8.19s/it]\n",
      "Embeddings: 100%|██████████| 3/3 [00:26<00:00,  8.82s/it]\n",
      "Embeddings: 100%|██████████| 4/4 [00:33<00:00,  8.44s/it]\n",
      "Embeddings: 100%|██████████| 2/2 [00:19<00:00,  9.76s/it]\n",
      "Languages: 100%|██████████| 9/9 [04:54<00:00, 32.72s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(words, language, model, batch_size=200):\n",
    "    embeddings = []\n",
    "    for batch in tqdm.tqdm(\n",
    "        range(0, len(words), batch_size),\n",
    "        desc=\"Embeddings\",\n",
    "        total=len(words) // batch_size + 1\n",
    "    ):\n",
    "        batch_embeddings = model.predict(words[batch:batch + batch_size], source_lang=f\"{language}_Latn\")\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "language_topic_word_embeddings = {}\n",
    "\n",
    "for language, words in tqdm.tqdm(language_topic_words.items(), desc=\"Languages\"):\n",
    "    if language == \"ven\":\n",
    "        embedding_language = \"sna\"\n",
    "    else:\n",
    "        embedding_language = language\n",
    "    embeddings = get_embeddings(words, embedding_language, t2vec_model)\n",
    "\n",
    "    for word, embedding in zip(words, embeddings):\n",
    "        language_topic_word_embeddings[(language, word)] = embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tensors to lists\n",
    "language_topic_word_embeddings = {\n",
    "    key: value.tolist()\n",
    "    for key, value in language_topic_word_embeddings.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe\n",
    "topic_words_df = pd.DataFrame([\n",
    "    {\n",
    "        \"language\": language,\n",
    "        \"word\": word,\n",
    "        \"embedding\": embedding\n",
    "    }\n",
    "    for (language, word), embedding in language_topic_word_embeddings.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>word</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zul</td>\n",
       "      <td>emva</td>\n",
       "      <td>[0.0037508714012801647, 0.002158257644623518, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zul</td>\n",
       "      <td>kubo</td>\n",
       "      <td>[0.006009371485561132, 0.002523238305002451, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zul</td>\n",
       "      <td>ibe</td>\n",
       "      <td>[0.007078166585415602, 0.007814861834049225, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zul</td>\n",
       "      <td>ukwelashwa</td>\n",
       "      <td>[0.0038973756600171328, 0.00260126288048923, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zul</td>\n",
       "      <td>ngaphezulu</td>\n",
       "      <td>[0.002381658647209406, -0.0013715632958337665,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language        word                                          embedding\n",
       "0      zul        emva  [0.0037508714012801647, 0.002158257644623518, ...\n",
       "1      zul        kubo  [0.006009371485561132, 0.002523238305002451, -...\n",
       "2      zul         ibe  [0.007078166585415602, 0.007814861834049225, -...\n",
       "3      zul  ukwelashwa  [0.0038973756600171328, 0.00260126288048923, -...\n",
       "4      zul  ngaphezulu  [0.002381658647209406, -0.0013715632958337665,..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to parquet\n",
    "topic_words_df.to_parquet(f\"{data_directory}/topic_word_vectors.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df\n",
    "topic_words_df = pd.read_parquet(f\"{data_directory}/topic_word_vectors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word vectors by grouping over words and taking the mean of the embeddings\n",
    "topic_word_vectors_lang_ind = topic_words_df.groupby(\"word\")[\"embedding\"].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_length = len(topic_word_vectors_lang_ind.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word_vectors = {}\n",
    "\n",
    "def word_vector(word):\n",
    "    if word in topic_word_vectors_lang_ind:\n",
    "        embedding = topic_word_vectors_lang_ind[word]\n",
    "        return embedding\n",
    "        \n",
    "    if word.isnumeric():\n",
    "        return [0] * embedding_length\n",
    "            \n",
    "    if word in new_word_vectors:\n",
    "        return new_word_vectors[word]\n",
    "    \n",
    "\n",
    "    new_word_vectors[word] = t2vec_model.predict([word], source_lang=\"zul_Latn\")[0]\n",
    "    return new_word_vectors[word]\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:\n",
    "        return 0\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def topic_similarity(topic):\n",
    "    similarities = [\n",
    "        cosine_similarity(word_vector(word1), word_vector(word2))\n",
    "        for word1, word2 in itertools.combinations(topic, 2)\n",
    "    ]\n",
    "    score = np.mean(similarities)\n",
    "    return score\n",
    "\n",
    "\n",
    "def topics_similarity(topics):\n",
    "    return np.mean([\n",
    "        topic_similarity(topic)\n",
    "        for topic in topics\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilinguality scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_multilinguality(topic):\n",
    "    languages = set()\n",
    "    for word in topic:\n",
    "        if word not in words_languages:\n",
    "            continue\n",
    "        word_langs = words_languages[word]\n",
    "        languages.update(word_langs)\n",
    "\n",
    "    value = (len(languages) - 1) / (N_LANGUAGES - 1) \n",
    "    return value\n",
    "\n",
    "words_languages = {}\n",
    "for word, language in topic_words_df[[\"word\", \"language\"]].values:\n",
    "    if word in words_languages:\n",
    "        words_languages[word].add(language)\n",
    "    else:\n",
    "        words_languages[word] = {language}\n",
    "\n",
    "def topics_multilinguality(topics):\n",
    "    return np.mean([\n",
    "        topic_multilinguality(topic)\n",
    "        for topic in topics\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTC scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_mtc(topic):\n",
    "    sim_score = topic_similarity(topic)\n",
    "    mul_score = topic_multilinguality(topic)\n",
    "    harmonic_mean = 2 * sim_score * mul_score / (sim_score + mul_score)\n",
    "    return harmonic_mean\n",
    "\n",
    "def topics_mtc(topics):\n",
    "    return np.mean([\n",
    "        topic_mtc(topic)\n",
    "        for topic in topics\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_directory}/output.jsonl\") as f:\n",
    "    output = [json.loads(line) for line in f]\n",
    "\n",
    "output_df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>num_topics</th>\n",
       "      <th>topics</th>\n",
       "      <th>npmi</th>\n",
       "      <th>multilinguality</th>\n",
       "      <th>multilinguality_hard</th>\n",
       "      <th>topic_similarity</th>\n",
       "      <th>mtc_similarity</th>\n",
       "      <th>mtc_npmi</th>\n",
       "      <th>diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lda</td>\n",
       "      <td>50</td>\n",
       "      <td>[[emva, nabo, zonke, lawo, izinto, izicelo, lo...</td>\n",
       "      <td>0.064995</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>0.164644</td>\n",
       "      <td>0.824642</td>\n",
       "      <td>0.841352</td>\n",
       "      <td>0.121119</td>\n",
       "      <td>0.177191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lda</td>\n",
       "      <td>50</td>\n",
       "      <td>[[nama, lawa, ngendlela, ngayo, bakhe, ebusuku...</td>\n",
       "      <td>0.063930</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>0.167813</td>\n",
       "      <td>0.825161</td>\n",
       "      <td>0.857130</td>\n",
       "      <td>0.119488</td>\n",
       "      <td>0.176744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lda</td>\n",
       "      <td>50</td>\n",
       "      <td>[[eka, no, c, kuchaza, umsebenzi, ngaso, eziny...</td>\n",
       "      <td>0.062965</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>0.164812</td>\n",
       "      <td>0.819475</td>\n",
       "      <td>0.841835</td>\n",
       "      <td>0.117630</td>\n",
       "      <td>0.181844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lda</td>\n",
       "      <td>50</td>\n",
       "      <td>[[la, bo, le, lakho, ukuqinisekisa, imithetho,...</td>\n",
       "      <td>0.061783</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>0.159666</td>\n",
       "      <td>0.822795</td>\n",
       "      <td>0.834839</td>\n",
       "      <td>0.115438</td>\n",
       "      <td>0.178834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lda</td>\n",
       "      <td>50</td>\n",
       "      <td>[[nama, south, of, africa, pretoria, ube, bakh...</td>\n",
       "      <td>0.063234</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>0.162014</td>\n",
       "      <td>0.823971</td>\n",
       "      <td>0.836761</td>\n",
       "      <td>0.117990</td>\n",
       "      <td>0.177611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  num_topics                                             topics  \\\n",
       "0   lda          50  [[emva, nabo, zonke, lawo, izinto, izicelo, lo...   \n",
       "1   lda          50  [[nama, lawa, ngendlela, ngayo, bakhe, ebusuku...   \n",
       "2   lda          50  [[eka, no, c, kuchaza, umsebenzi, ngaso, eziny...   \n",
       "3   lda          50  [[la, bo, le, lakho, ukuqinisekisa, imithetho,...   \n",
       "4   lda          50  [[nama, south, of, africa, pretoria, ube, bakh...   \n",
       "\n",
       "       npmi  multilinguality  multilinguality_hard  topic_similarity  \\\n",
       "0  0.064995           0.8875              0.164644          0.824642   \n",
       "1  0.063930           0.9125              0.167813          0.825161   \n",
       "2  0.062965           0.8925              0.164812          0.819475   \n",
       "3  0.061783           0.8775              0.159666          0.822795   \n",
       "4  0.063234           0.8800              0.162014          0.823971   \n",
       "\n",
       "   mtc_similarity  mtc_npmi  diversity  \n",
       "0        0.841352  0.121119   0.177191  \n",
       "1        0.857130  0.119488   0.176744  \n",
       "2        0.841835  0.117630   0.181844  \n",
       "3        0.834839  0.115438   0.178834  \n",
       "4        0.836761  0.117990   0.177611  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df[\"npmi\"] = output_df[\"topics\"].apply(npmi_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows that have NaN in npmi and calculate npmi\n",
    "for i, row in output_df[output_df[\"npmi\"].isna()].iterrows():\n",
    "    output_df.loc[i, \"npmi\"] = npmi_topics(row[\"topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output: 100%|██████████| 150/150 [00:34<00:00,  4.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# apply to the topics\n",
    "for output_obj in tqdm.tqdm(output, desc=\"output\"):\n",
    "    topic_similarity_score = topics_similarity(output_obj[\"topics\"])\n",
    "    output_obj[\"topic_similarity\"] = topic_similarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw multilinguality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df[\"multilinguality\"] = output_df[\"topics\"].apply(topics_multilinguality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df[\"topic_similarity\"] = output_df[\"topics\"].apply(topics_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate harmonic mean of multilinguality_easy and topic_similarity call it mtc_similarity\n",
    "output_df[\"mtc_similarity\"] = output_df[\"topics\"].apply(topics_mtc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_code-idiVf7sk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
