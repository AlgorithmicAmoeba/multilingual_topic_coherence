{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leipzig Corpora Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import pandas  as pd\n",
    "import sklearn.feature_extraction.text\n",
    "import tqdm\n",
    "\n",
    "data_dir = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find sentences.txt in all directories\n",
    "sentence_files = glob.glob(data_dir + \"af_lang/*/*sentences.txt\")\n",
    "\n",
    "data = []\n",
    "\n",
    "for sentence_file in sentence_files:\n",
    "    with open(sentence_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            _, text = line.split(\"\\t\")\n",
    "            data.append({\n",
    "                \"text\": text,\n",
    "                \"language\": sentence_file.split(\"/\")[-2].split(\"-\")[0]\n",
    "            })\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_by_lang = {\n",
    "    lang: [d for d in data if d[\"language\"] == lang]\n",
    "    for lang in lang_counts.keys()\n",
    "}\n",
    "\n",
    "len(data_by_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darren/.local/share/virtualenvs/phd_code-idiVf7sk/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sonar.inference_pipelines.text\n",
    "\n",
    "t2vec_model = sonar.inference_pipelines.text.TextToEmbeddingModelPipeline(\n",
    "    encoder=\"text_sonar_basic_encoder\",\n",
    "    tokenizer=\"text_sonar_basic_encoder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings\n",
    "for lang, lang_data in data_by_lang.items():\n",
    "    sentences = [d[\"text\"] for d in lang_data]\n",
    "    if \"embedding\" in lang_data[0]:\n",
    "        continue\n",
    "\n",
    "    if lang == \"ven\":\n",
    "        lang = \"sna\"\n",
    "    batch_size = 128\n",
    "    embeddings = []\n",
    "    for i in tqdm.tqdm(range(0, len(sentences), batch_size), desc=f\"Embedding {lang}\"):\n",
    "        embeddings += t2vec_model.predict(sentences[i:i+batch_size], source_lang=f\"{lang}_Latn\")\n",
    "    for d, emb in zip(lang_data, embeddings):\n",
    "        d[\"embedding\"] = emb.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to parquet\n",
    "df = pd.DataFrame(data)\n",
    "df.to_parquet(f\"{data_dir}/data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get BoW representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet(f\"{data_dir}/data.parquet\").to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "languages = set(d[\"language\"] for d in data)\n",
    "\n",
    "# get word frequencies by language\n",
    "word_freqs_by_lang = {\n",
    "    lang: collections.Counter()\n",
    "    for lang in languages\n",
    "}\n",
    "\n",
    "def get_clean_words(text):\n",
    "    # remove punctuation\n",
    "    text = \"\".join([c for c in text if c.isalnum() or c.isspace()])\n",
    "    words = text.lower().split()\n",
    "    # remove words that are just numbers\n",
    "    words = [word for word in words if not word.isnumeric()]\n",
    "    return words\n",
    "\n",
    "for d in data:\n",
    "    lang = d[\"language\"]\n",
    "\n",
    "    words = get_clean_words(d[\"text\"])\n",
    "    \n",
    "    word_freqs_by_lang[lang].update(words)\n",
    "\n",
    "# get common words by language\n",
    "common_words_by_lang = {\n",
    "    lang: {\n",
    "        word: freq\n",
    "        for word, freq in word_freqs.items()\n",
    "        # if freq < 1000\n",
    "    }\n",
    "    for lang, word_freqs in word_freqs_by_lang.items()\n",
    "}\n",
    "\n",
    "# sorted by frequency\n",
    "common_words_by_lang = {\n",
    "    lang: dict(sorted(word_freqs.items(), key=lambda x: -x[1]))\n",
    "    for lang, word_freqs in common_words_by_lang.items()\n",
    "}\n",
    "\n",
    "# get 600 most common words by language\n",
    "common_words_by_lang = {\n",
    "    lang: dict(list(word_freqs.items())[:600])\n",
    "    for lang, word_freqs in common_words_by_lang.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4412"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct vocabulary\n",
    "vocabulary = dict()\n",
    "for lang, words in common_words_by_lang.items():\n",
    "    for word in words:\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = {\n",
    "                \"idx\": len(vocabulary),\n",
    "                \"languages\": []\n",
    "            }\n",
    "\n",
    "        vocabulary[word][\"languages\"].append(lang)\n",
    "\n",
    "\n",
    "# save vocabulary\n",
    "with open(f\"{data_dir}/vocabulary.json\", \"w\") as f:\n",
    "    json.dump(vocabulary, f)\n",
    "\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [01:03<00:00, 1411.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# construct bag of words vector for each document\n",
    "for d in tqdm.tqdm(data):\n",
    "    words = get_clean_words(d[\"text\"])\n",
    "    bow = collections.Counter(words)\n",
    "    d[\"bow\"] = [bow[word] for word in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# add cleaned words to data without stop words\n",
    "for d in data:\n",
    "    clean_words = get_clean_words(d[\"text\"])\n",
    "    d[\"clean_words\"] = clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(f\"{data_dir}/data.parquet\", chunk_size=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_parquet(f\"{data_dir}/data.parquet\", chunk_size=30000).to_dict(orient=\"records\")\n",
    "\n",
    "transformer = sklearn.feature_extraction.text.TfidfTransformer()\n",
    "X = [d[\"bow\"] for d in data]\n",
    "X_tfidf = transformer.fit_transform(X)\n",
    "\n",
    "# save tfidf vectors in data\n",
    "for i in range(len(data)):\n",
    "    data[i][\"tfidf\"] = X_tfidf[counter].toarray().tolist()[0]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "df.to_parquet(f\"{data_dir}/data.parquet\", chunk_size=30000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_code-idiVf7sk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
